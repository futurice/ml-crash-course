{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEkKoR8_qv6t"
   },
   "source": [
    "# Content based recommendation\n",
    "\n",
    "Content based recommendation egnines compare items and find items similar to a query item. In this exercise, we will learn to measure the similarity of textual documents and build  a recommender for news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "EZsvmtOHftcq"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions we are going to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def row_as_array(M, i):\n",
    "  \"\"\"Get the i:th row of a sparse matrix M and return it as a 1-D numpy array\"\"\"\n",
    "  return np.squeeze(np.asarray(tf[i, :].todense()))\n",
    "\n",
    "def row_as_series(M, i, vectorizer):\n",
    "  return pd.Series(row_as_array(M, i), index=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02c8A7iqrmR7"
   },
   "source": [
    "## News article dataset\n",
    "\n",
    "Let's load a set of Reuters news articles. We'll need to clean the data a bit and extract the actual text of the articles to a variable called `documents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "x_SSnCXlfwx_"
   },
   "outputs": [],
   "source": [
    "dataset = requests.get('http://ana.cachopo.org/datasets-for-single-label-text-categorization/r52-train-all-terms.txt?attredirects=0').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xVb5oSCdgUUO"
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "for line in dataset.decode('UTF-8').split('\\n'):\n",
    "  # the part before the tab is a document class, which we ignore here\n",
    "  fields = line.strip().split('\\t', 1)\n",
    "  if len(fields) == 2:\n",
    "    labels.append(fields[0])\n",
    "    documents.append(fields[1])\n",
    "\n",
    "labels = np.array(labels)\n",
    "documents = pd.Series(documents)\n",
    "\n",
    "print('Loaded {} documents'.format(len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The news articles have been grouped to these categories (each article belong to one of the categories):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrJR_4RBi9In"
   },
   "source": [
    "Let's print the first news articles to see how they look. Notice that punctuation and upper cases have already been removed in the data source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DcfcNuWdhDnp"
   },
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z0KqLhMsU7-"
   },
   "source": [
    "## Preprocessing text documents\n",
    "\n",
    "Most of the machine learning methods deal with number, not text. Therefore, text is commonly converted into numerical vectors for processing. A simple baseline method is to just count how many times each word appears in a document and collect the counts (called, the term frequencies) into a vector. The scikit-learn library provides a tool for performing the conversion.\n",
    "\n",
    "* The `ngram_range` parameter indicates that we want to have not just the individual words as tokens, but also sequences of 2 and 3 consequtive words.\n",
    "* The `min_df` parameter filters out tokens which appear less frequently than in 0.5% of the documents.\n",
    "* The `stop_words` parameter filters out common English words.\n",
    "\n",
    "See the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7uejlExhkP_w"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1), min_df=0.005, stop_words='english')\n",
    "vectorizer.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un4TZUjZuJy0"
   },
   "source": [
    "The `fit` method collects the tokens that appear in the documents and match the `ngram_range`, `min_df`, `stop_word` parameters.\n",
    "\n",
    "Let's see what kind of tokens it found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "K9PS_5p-uyrY"
   },
   "outputs": [],
   "source": [
    "print('Number of tokens: {}'.format(len(vectorizer.get_feature_names())))\n",
    "\n",
    "print()\n",
    "print('First 10 tokens:')\n",
    "vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKRp9OaMvKMI"
   },
   "source": [
    "The main functionality of the vectorizer is that we can convert text documents into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2GTmQHV0k6ZN"
   },
   "outputs": [],
   "source": [
    "tf = vectorizer.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DoAg03SmvaUr"
   },
   "source": [
    "`tf` is a matrix, where each row is a count vector for one document.\n",
    "\n",
    "Let's define a helper function for inspecting the rows and inspect the first document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimension of the tf matrix are {}\".format(tf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "z7kh4K-Mv0Zb"
   },
   "outputs": [],
   "source": [
    "x = row_as_series(tf, 0, vectorizer)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go0LeqZVwPAO"
   },
   "source": [
    "Most of the counts are zeros because one document contains only a subset of all words in the vocabulary.\n",
    "\n",
    "To show that it's not all zeroes, let's take a subset that includes some non-zero values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RTkxjSOiwOO6"
   },
   "outputs": [],
   "source": [
    "x['cocoa':'commission']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the news articles varies from a couple of words to over 300 words as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tf.sum(axis=1), bins=30)\n",
    "plt.xlabel('Number of words in a article')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to consider two documents similar just because they have similar number of words. Therefore, we normalize the term frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnormalized = normalize(tf.todense(), norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Human can't easily understand the tf matrix that has over 1000 dimensions. If we want to visualize the data, we need to reduce it to two dimensions. This will obviously lose some (or actually quite a lot of) information in the data, but PCA will keep the most important dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimreduction = PCA(n_components=2)\n",
    "Xreduced = dimreduction.fit_transform(Xnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimensionality of the original tf matrix: {}\".format(tf.shape))\n",
    "print(\"The dimensionality after PCA: {}\".format(Xreduced.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data after the dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 16))\n",
    "plt.plot(Xreduced[:, 0], Xreduced[:, 1], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some interesting structure in the data, at least three separated clusters.\n",
    "\n",
    "To better understand what kind of articles belong to the clusters, we can overlay the beginning of a few random documents on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 16))\n",
    "plt.plot(Xreduced[:, 0], Xreduced[:, 1], '.')\n",
    "\n",
    "indexes = random.sample(range(tf.shape[0]), 10)\n",
    "for i in indexes:\n",
    "    label = ' '.join(documents[i].split(' ')[:10])\n",
    "    plt.text(Xreduced[i, 0], Xreduced[i, 1], label, size='xx-large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(Xreduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering algorithm assigned a cluster index to each sample. Let's see the first few cluster indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the clustering result by drawing each data point with the color corresponding the its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_colors = 'bgrcmyk'\n",
    "colors = [available_colors[i % len(available_colors)] for i in kmeans.labels_]\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "for i in range(Xreduced.shape[0]):\n",
    "    plt.plot(Xreduced[i, 0], Xreduced[i, 1], '.', color=colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering the two dimensional data is a toy example because you can already see the structure by looking at the visualization.\n",
    "\n",
    "In reality, clustering is applied to high dimensional data, like the new article data before the dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(n_clusters=6)\n",
    "kmeans2.fit(Xnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in range(kmeans2.n_clusters):\n",
    "    indexes = random.sample(np.where(kmeans2.labels_ == cl)[0].tolist(), 5)\n",
    "\n",
    "    print(\"Random documents from cluster {}\".format(cl))\n",
    "    print('-'*40)\n",
    "    \n",
    "    for i in indexes:\n",
    "        print(documents[i])\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Vary the number of clusters (`n_clusters`) and both analyses (the clustering of the dimensionality reduced data and the clustering of the original data). How large or small number of clusters still leads to sensible clusterings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t19-lxAhyHEe"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "The CountVectorizer tends to overweight very common words. A better way is to re-weight words that appear in many documents by multiplying the raw term frequencies (TF) with (some function of) the inverse of in how many documnets a term occurs (inverse document frequencies, IDF). The resulting vectors are called TF-IDF vectors. Scikit-learn provides [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) for this task. It is almost a drop-in replacement for the CountVectorizer that was used above.\n",
    "\n",
    "Repeat the above analysis with TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "drbaq76rp1LR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Content based recommendation.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
